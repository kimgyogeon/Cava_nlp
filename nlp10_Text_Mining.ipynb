{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp10_Text Mining",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPuAP1cNGrNbBw9jFHurHZF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimgyogeon/Cava_nlp/blob/main/nlp10_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpT357vr7Lfx"
      },
      "source": [
        "**NLP, 텍스트 분석**\r\n",
        "\r\n",
        "* Natural Language Processing: 기계가 인간의 언어를 이해하고 해석하는데 중점. 기계번역, 질의 응답시스템\r\n",
        "*  텍스트 분석: 비정형 텍스트에서 의미있는 정보를 추출하는것에 중점\r\n",
        "* NLP는 텍스트 분석을 향상하게 하는 기반기술\r\n",
        "* NLP와 텍스트 분석의 근간에는 머신러닝이 존재, 과거 언어적인 툴 기반 시스템에서 텍스트 데이터기반으로 모델을 학습하고 예측\r\n",
        "\r\n",
        "* 텍스트 분석은 머신러닝, 언어이해, 통계등을 활용한 모델 수립, 정보 추출을 통해 인사이트 및 예측 분석 등의 분석 작업 수행\r\n",
        "\r\n",
        "  1.   텍스트분류: 신문기사 카테고리 분류, 스팸메일 검출 프로그램, 지도학습\r\n",
        "  2.   감성분석: 감정/판단/믿음/의견/기분 등의 주관적 요소 분석. 소셜미디어 감정분석, 영화리뷰, 여론조사, 의견분석, 지도학습, 비지도학습\r\n",
        "  3. 텍스트 요약: 텍스트 내에서 중요한 주제나 중심 사상을 추출, 토픽 모델링\r\n",
        "  4.  텍스트 군집화와 유사도 측정: 비슷한 유형의 문서에 대해 군집화 수행, 비지도학습\r\n",
        "\r\n",
        "**Text 분석 수행 프로세스**\r\n",
        "*  텍스트 정규화\r\n",
        "    -클랜징, 토큰화,필터링/스톱워드제거/철자수정,Stemming, Lemmatization\r\n",
        "\r\n",
        "*  피처 벡터화 변환\r\n",
        "    -Bag of Words: Count 기반, TF-IDF기반\r\n",
        "    -Word2Vec\r\n",
        "*  ML모델 수립 및 학습/예측/평가\r\n",
        "\r\n",
        "**텍스트 전처리**-텍스트 정규화\r\n",
        "*  클렌징 : 분석에 방해되는 불필요한 문자, 기호를 사전에제거, HTML,XML태그나 특정 기호\r\n",
        "*  토큰화: 문서에서 문장을 분리하는 문장 토큰화와 문장에서 단어를 토큰으로 분리하는 단어 토큰화\r\n",
        "*  필터링/스톱워드 제거/ 철자 수정: 분석에 큰 의미가 없는 단어를 제거\r\n",
        "*  Stemming, Lemmatization: 문법적 또는 의미적으로 변화하는 단어의 원형을 찾음\r\n",
        "    * Stemming은 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용\r\n",
        "    * Lemmatization 이 Stemming 보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-SrqwIDC9nv",
        "outputId": "aeac6ff6-6149-467e-d1ab-ae7d11b7903b"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfD2rydmDHZD",
        "outputId": "fb867bd7-c2de-4d08-b0d1-e86664428ada"
      },
      "source": [
        "# 문장 토큰화 (sent tokenize) :마침표, 개행문자(\\n), 정규표현식\r\n",
        "from nltk import sent_tokenize\r\n",
        "text_sample = 'i love you. \\\r\n",
        "you can see something. \\\r\n",
        "i don\"t care.'\r\n",
        "sentences = sent_tokenize(text = text_sample)\r\n",
        "print(sentences)\r\n",
        "print(type(sentences), len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i love you.', 'you can see something.', 'i don\"t care.']\n",
            "<class 'list'> 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1_jCDOSEKT0",
        "outputId": "fa408f76-5bb7-497b-ddbc-987c49bef375"
      },
      "source": [
        "# 단어 토큰화(word_tokenize) : 공백, 콤마, 마침표, 개행문자, 정규표현식\r\n",
        "from nltk import word_tokenize\r\n",
        "\r\n",
        "sentence = 'i love you.'\r\n",
        "words = word_tokenize(sentence)\r\n",
        "print(words)\r\n",
        "print(type(words),len(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'love', 'you', '.']\n",
            "<class 'list'> 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6VrGFzMEzji",
        "outputId": "5d2f913e-664e-46b9-ab2b-ee74b102cbec"
      },
      "source": [
        "# 문서에 대해서 모든 단어를 토큰화\r\n",
        "from nltk import word_tokenize, sent_tokenize\r\n",
        "def tokenize_text(text):\r\n",
        "  sentences = sent_tokenize(text) #문장별 분리토큰\r\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences] #문장별 단어 토큰화\r\n",
        "  return word_tokens\r\n",
        "\r\n",
        "word_tokens = tokenize_text(text_sample)\r\n",
        "print(word_tokens)\r\n",
        "print(type(word_tokens), len(word_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['i', 'love', 'you', '.'], ['you', 'can', 'see', 'something', '.'], ['i', 'don', \"''\", 't', 'care', '.']]\n",
            "<class 'list'> 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ge2beEtHmML",
        "outputId": "205c7c56-0c7e-426a-cf66-491fbe3679af"
      },
      "source": [
        "# 스톱워드 제거 : the, is, a, will와 같이 문맥적으로 큰 의미가 없는 단어를 제거\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdWonPvBIFNU",
        "outputId": "af57bb90-111b-4148-8bd2-5527beed441a"
      },
      "source": [
        "#NLTK english stopwords 갯수 확인\r\n",
        "print(len(nltk.corpus. stopwords.words('english')))\r\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv__Ga4fI6J2",
        "outputId": "7a3e2898-bef9-4c31-8687-5d5168331184"
      },
      "source": [
        "# stopwords 필터링을 통한 제거\r\n",
        "import nltk\r\n",
        "stopwords = nltk.corpus.stopwords.words('english')\r\n",
        "all_tokens = []\r\n",
        "for sentence in word_tokens:\r\n",
        "  filtered_words = []\r\n",
        "  for word in sentence:\r\n",
        "    word = word.lower()\r\n",
        "    if word not in stopwords:\r\n",
        "      filtered_words.append(word)\r\n",
        "  all_tokens.append(filtered_words)\r\n",
        "print(all_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['love', '.'], ['see', 'something', '.'], [\"''\", 'care', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKohiCSeKUjl",
        "outputId": "ba10e3ed-525c-41fa-eac4-c087859e003d"
      },
      "source": [
        "# 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법\r\n",
        "# Stemmer(LancasterStemmer)\r\n",
        "from nltk.stem import LancasterStemmer\r\n",
        "stemmer = LancasterStemmer()\r\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\r\n",
        "print(stemmer.stem('amusing'),stemmer.stem('aumses'),stemmer.stem('amused'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus aums amus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgCwfRJ3MnrN",
        "outputId": "ec123a07-0e0f-453a-e938-9c317f1875df"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYsfFM_mLOSV",
        "outputId": "15e79b78-253e-4f5f-8e20-7071a7696536"
      },
      "source": [
        "# Lemmatization(WordNetLemmatizer): 정확한 원형 단어 추출을 위해 단어의 품사를 직접 입력\r\n",
        "\r\n",
        "from nltk.stem. wordnet import WordNetLemmatizer\r\n",
        "\r\n",
        "lemma = WordNetLemmatizer()\r\n",
        "print(lemma.lemmatize('working','v'),lemma.lemmatize('works','v'),lemma.lemmatize('worked','v'))\r\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('aumses','v'),lemma.lemmatize('amused','v'))\r\n",
        "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fancier','a'),lemma.lemmatize('fancier','a'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amuse aumses amuse\n",
            "fancy fancy fancy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQBgdknAleEt"
      },
      "source": [
        "GPU vs CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "560dy-Wlr_EX",
        "outputId": "07641d1f-c45c-4404-eab0-f74d97c178d9"
      },
      "source": [
        "import numpy as np\r\n",
        "num_samples = 100\r\n",
        "height = 71\r\n",
        "width = 71\r\n",
        "num_classes = 100\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from keras.applications import Xception\r\n",
        "import datetime\r\n",
        "start = datetime.datetime.now()\r\n",
        "\r\n",
        "model = Xception(weights = None,\r\n",
        "                 input_shape=(height,width,3),\r\n",
        "                 classes=num_classes)\r\n",
        "model.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer='rmsprop')\r\n",
        "x=np.random.random((num_samples,height,width,3))\r\n",
        "y=np.random.random((num_samples,num_classes))\r\n",
        "\r\n",
        "model.fit(x,y,epochs=3,batch_size=16)\r\n",
        "model.save('my_model.h5')\r\n",
        "end = datetime.datetime.now()\r\n",
        "time_delta = end - start"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "7/7 [==============================] - 41s 178ms/step - loss: 234.6610\n",
            "Epoch 2/3\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 245.7527\n",
            "Epoch 3/3\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 246.6586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkkoTI2psD3C"
      },
      "source": [
        "print('걸린시간:{}초'.format(time_delta.seconds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC35w-2gsG9U"
      },
      "source": [
        "import numpy as np\r\n",
        "num_samples = 100\r\n",
        "height = 71\r\n",
        "width = 71\r\n",
        "num_classes = 100\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from keras.applications import Xception\r\n",
        "import datetime\r\n",
        "start = datetime.datetime.now()\r\n",
        "\r\n",
        "with tf.device('/cpu:0'):\r\n",
        "  model = Xception(weights = None,\r\n",
        "                  input_shape=(height,width,3),\r\n",
        "                  classes=num_classes)\r\n",
        "  model.compile(loss='categorical_crossentropy',\r\n",
        "                optimizer='rmsprop')\r\n",
        "  x=np.random.random((num_samples,height,width,3))\r\n",
        "  y=np.random.random((num_samples,num_classes))\r\n",
        "\r\n",
        "  model.fit(x,y,epochs=3,batch_size=16)\r\n",
        "  model.save('my_model.h5')\r\n",
        "end = datetime.datetime.now()\r\n",
        "time_delta = end - start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVjPxXwFsK3E"
      },
      "source": [
        "\r\n",
        "print('걸린시간:{}초'.format(time_delta.seconds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kCW5FQgsLhK"
      },
      "source": [
        "피처 벡터화 : One-hot encoding Bag of Words : 문맥이나 순서를 무시하고 일괄적으로 단어에 대한 빈도 값을 부여해 피처 값을 추출하는 모델.\r\n",
        "단점 : 문맥 의미 반영 부족, 희소 행렬 문제.\r\n",
        "\r\n",
        "BOW에서 피처 벡터화 : 모든 단어를 컬럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것.\r\n",
        "피처 벡터화 방식 : 카운트 기반, TF-IDF(Term Frequency - Inverse Document Frequency) 기반 벡터화\r\n",
        "카운트 벡터화 : 카운트 값이 높을수록 중요한 단어로 인식. 특성상 자주 사용되는 보편적인 단어까지 높은 값 부여\r\n",
        "TF-IDF : 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서 패널티 부여. '빈번하게', '당연히', '조직', '업무' 등\r\n",
        "파라미터\r\n",
        "\r\n",
        "  -max_df : 너무 높은 빈도수를 가지는 단어 피처를 제외\r\n",
        "\r\n",
        "  -min_df : 너무 낮은 빈도수를 가지는 단어 피처를 제외\r\n",
        "\r\n",
        "  -max_features : 추출하는 피처의 개수를 제한하며 정수로 값을 지정\r\n",
        "\r\n",
        "  -stop_words : 'english'로 지정하면 스톱 워드로 지정된 단어는 추출에서 제외\r\n",
        "\r\n",
        "  -n_gram_range : 튜플 형태로 (범위 최솟값, 범위 최댓값)을 지정\r\n",
        "\r\n",
        "  -analyzer : 피처 추출을 수행하는 단위. 디폴트는 'word'\r\n",
        "\r\n",
        "  -token_pattern : 토큰화를 수행하는 정규 표현식 패턴을 지정\r\n",
        "\r\n",
        "  -tokenizer : 토큰화를 별도의 커스텀 함수로 이용시 적용\r\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meuvsae_sPHB"
      },
      "source": [
        "# ndarray 객체 생성\r\n",
        "import numpy as np\r\n",
        "dense = np.array([[3,0,1],[0,2,0]])\r\n",
        "dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QiOHtrasgH3"
      },
      "source": [
        "# 희소행렬 - COO 형식\r\n",
        "from scipy import sparse\r\n",
        "data = np.array([3,1,2]) # 0이 아닌 데이터\r\n",
        "row_pos = np.array([0,0,1])\r\n",
        "col_pos = np.array([0,2,1])\r\n",
        "sparse_coo = sparse.coo_matrix((data,(row_pos,col_pos)))\r\n",
        "print(sparse_coo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGiST2UCshrk"
      },
      "source": [
        "# 희소행렬 - COO vs CSR 형식\r\n",
        "dense2 = np.array([[0,0,1,0,0,5],\r\n",
        "                  [1,4,0,3,2,5],\r\n",
        "                  [0,6,0,3,0,0],\r\n",
        "                  [2,0,0,0,0,0],\r\n",
        "                  [0,0,0,7,0,8],\r\n",
        "                  [1,0,0,0,0,0]])\r\n",
        "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\r\n",
        "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\r\n",
        "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\r\n",
        "# COO 형식으로 변환\r\n",
        "sparse_coo = sparse.coo_matrix((data2,(row_pos,col_pos)))\r\n",
        "print(sparse_coo)\r\n",
        "print(sparse_coo.toarray())\r\n",
        "print()\r\n",
        "# CSR 형식으로 변환\r\n",
        "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\r\n",
        "row_pos_ind = np.array([0,2,7,9,10,12,13])\r\n",
        "sparse_csr = sparse.csr_matrix((data2,col_pos,row_pos_ind))\r\n",
        "print(sparse_csr)\r\n",
        "print(sparse_csr.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa8vYFlgsjph"
      },
      "source": [
        "# DictVectorizer : 문서에서 단어의 사용빈도를 나타내는 딕셔너리 정보를 입력받아 \r\n",
        "# BOW 인코딩한 수치 벡터로 전환\r\n",
        "from sklearn.feature_extraction import DictVectorizer\r\n",
        "v = DictVectorizer(sparse=False)\r\n",
        "D = [{'A':1,'B':2},{'B':3,'C':1}]\r\n",
        "X = v.fit_transform(D)\r\n",
        "print(X)\r\n",
        "print(v.feature_names_)\r\n",
        "print(v.vocabulary_)\r\n",
        "print(v.transform({'C':4,'D':3}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U7IU4Dvsps1"
      },
      "source": [
        "# CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "corpus = [\r\n",
        "    'This is the first document.',\r\n",
        "    'This is the second document',\r\n",
        "    'And the third one.',\r\n",
        "    'Is this the first document?',\r\n",
        "    'The last document?',\r\n",
        "]\r\n",
        "vect = CountVectorizer()\r\n",
        "vect.fit(corpus)\r\n",
        "print(vect.get_feature_names())\r\n",
        "print()\r\n",
        "print(vect.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L-xF62YstH0"
      },
      "source": [
        "print(vect.transform(['This is the second document']).toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRyWpz3fsutA"
      },
      "source": [
        "\r\n",
        "print(vect.transform(['Something completely new.']).toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3IS4LQQswEY"
      },
      "source": [
        "print(vect.transform(corpus).toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4qkzSKPsynL"
      },
      "source": [
        "# Stop Words는 문서에서 단어장을 생성할 때 무시할 수 있는 단어. \r\n",
        "# 보통 영어의 관사, 접속사, 한국어의 조사 등\r\n",
        "\r\n",
        "vect = CountVectorizer(stop_words=['and','is','the','this']).fit(corpus)\r\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L10EDLB1s0YZ"
      },
      "source": [
        "vect = CountVectorizer(stop_words='english').fit(corpus)\r\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ard-0Ig4tGAp"
      },
      "source": [
        "# analyzer, tokenizer, token_pattern 등의 인수로 사용할 토큰 생성기를 선택\r\n",
        "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\r\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii1YsPm3tHtm"
      },
      "source": [
        "vect = CountVectorizer(token_pattern='t\\w+').fit(corpus)\r\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzvuFuettJS1"
      },
      "source": [
        "# n-그램은 단어장 생성에 사용할 토큰의 크기 결정\r\n",
        "vect = CountVectorizer(ngram_range=(1,2)).fit(corpus)\r\n",
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyrDnpGjtKqZ"
      },
      "source": [
        "vect = CountVectorizer(ngram_range=(1,2),token_pattern='t\\w+').fit(corpus)\r\n",
        "vect.vocabulary_\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdlKiEnUtUfs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI8k0YA3tNVL"
      },
      "source": [
        "TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소\r\n",
        "문서 d(document)와 단어 t 에 대해 다음과 같이 계산\r\n",
        "\r\n",
        "tf-idf(d,t)=tf(d,t)⋅idf(t)\r\n",
        "\r\n",
        "tf(d,t): term frequency. 특정한 단어의 빈도수\r\n",
        "\r\n",
        "idf(t) : inverse document frequency. 특정한 단어가 들어 있는 문서의 수에 반비례하는 수\r\n",
        "\r\n",
        "n : 전체 문서의 수\r\n",
        "\r\n",
        "df(t) : 단어 t 를 가진 문서의 수\r\n",
        "\r\n",
        "idf(d,t)=log(n/(1+df(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NXH0bYRtYA8"
      },
      "source": [
        "corpus = [\"\"\"\r\n",
        "The Corpus of Contemporary American English (COCA) is the only large, \r\n",
        "genre-balanced corpus of American English. COCA is probably the most \r\n",
        "widely-used corpus of English, and it is related to many other corpora of \r\n",
        "English that we have created, which offer unparalleled insight into variation \r\n",
        "in English.\r\n",
        "\"\"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRKTVOjmtYpm"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "tfidv = TfidfVectorizer(stop_words='english').fit(corpus)\r\n",
        "print(tfidv.get_feature_names())\r\n",
        "print()\r\n",
        "print(tfidv.vocabulary_)\r\n",
        "print()\r\n",
        "print(tfidv.transform(corpus).toarray())\r\n",
        "print(tfidv.transform(['This is the second document']).toarray())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}